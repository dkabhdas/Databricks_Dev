{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"ingestion/ingestion-patterns/SAP-BW-Ingestion-pattern/","text":"SAP BW Data Incremental Ingestion to LEGO Nexus Prerequisites Review the principles and guidelines to publish data from SAP BW to Lego Nexus outlined in data-matters Baseplate document. Data products should be registered using the data product registration template as described in the registration guide . If a BI engineering product team doesn't have an application in Architecture Gateway , they need to register one (or more if necessary) so that the source of the data is correctly represented and linked to the correct digital product. User guide to Create an application in Architecture Gateway. Prior to commencing the implementation of the CDC pipeline in Databricks, it is essential to configure the S3 bucket and establish a connection by linking your S3 bucket through the External location. The process for bringing your own bucket, requesting storage credentials and external location for a S3 bucket is outlined in detail within the Baseplate BYOB document. Architectural flow diagram Incremental Extraction and landing it in S3 Operational Data Provisioning (ODP) and Delta Queue (ODQ) based Extraction : Operational Data Provisioning (ODP) supports extraction and replication scenarios for various target applications and incorporates delta mechanisms in these scenarios. When dealing with a delta procedure, data from a source (referred to as the ODP Provider) is automatically written to a delta queue, known as the Operational Delta Queue (ODQ), through an update process or passed to the delta queue via an extractor interface. The ODQ effectively tracks new and modified records since the last extraction, facilitating incremental data updates. ODP Context or Provider An ODP context represents a source of ODPs. Context identifiers are present for all technologies whose analytical views can be exposed as ODPs. Currently, the following ODP contexts are available (depending on release): Provider Name Description ABAP_CDS ABAP Core Data Services BW SAP NetWeaver / BW/4HANA BYD SAP Business ByDesign. Data from MDAV extracted via ODP. Implemented via SOAP Web Service. ESH Search and operational analytics HANA SAP HANA Information View SAPI SAP Service Application Programming Interface (S-API) for SAP DataSources SLT~ SAP Landscape Transformation Replication Server In our extraction process, we will utilize the BW context and retrieve data from the BW DataStore . ODP Subscribers or Consumers Currently, the following subscriber types are available (depending on release): Subscribers Description SAP_BW/4HANA SAP NetWeaver Business Warehouse DS SAP Business Objects Data Services TREX_ES SAP NetWeaver Embedded Analytics. Query is defined on transient provider, derived from ODP RODPS_REPL_TEST Created by executing report RODPS_REPL_TEST (in transaction SE38) RSODP_ODATA Open Data Protocol (OData) HANA_SDI SAP HANA Smart Data Integration We will be using SAP Data Services as consumer. Extraction using SAP Data Services : Data Services has seamless integration capabilities with SAP ODP-enabled data sources. In Lego BW system, most of the BW data store objects(DSO) are ODP enabled, and SAP Data Services can use extractors for data extraction through the ODP API framework. To extract data from the BW system using extractors, you should have a Datastore configured as the source in Data Services with BW ODP context. To import the required ODP-enabled object, follow these steps: Double-click on the Datastore to view the list of ODP-enabled objects. Identify the necessary ODP extractors from the list. Right-click on the desired ODP extractor and choose the \"Import\" option. During import provide the consumer name and project name and CDC Mode . When importing any ODP, one can choose between query mode or CDC mode. For incremental extraction choose CDC. Each of these projects will be a separate subscription in the source system, which you can view in the T-CODE ODQMON. If the file location isn't already created for that S3 landing bucket then reate a cloud storage file location to land the file in the landing S3 bucket. The team taking the role of the Data Producer has their own AWS Account. If the S3 bucket for landing data hasn't been created yet, please refer to the 'Create AWS Bucket' section in the Baseplate BYOB document. create an IAM user in the AWS account and Set permissions to read and write files from S3. Before the DataFlow, a scipt can be used to create CSV file names with a dynamic timestamp. Create a Data Services data flow that uses an ODP (Operational Data Provisioning) object as the source and exports the data to a specified file location. The way we use MAP CDC Operation depends on what we need \u2013 whether to send all the changes that happened over time or just the current information. This choice is based on the type of extractor we're using. The \"File name(s)\" points output of the script in the step no 5 to create a dynamic file name with a timestamp ($Name1) If the initial load is set to 'Yes,' it will import all the data during the first execution. Subsequently, for incremental loads, it should be set to 'No' after the initial execution. The sample output file contains a column labeled ODQ_CHANGEMODE , serving as an indicator for operational types. The potential values include: C - New Record U - Updated Record D - Deleted Record Data Load into LEGO Nexus Bronze table from S3 Implement CDC (Change Data Capture) Pipeline With Delta Lake : Delta Lake is designed to support CDC workloads by offering support for UPDATE, DELETE, and MERGE operations. Additionally, Delta tables can facilitate CDC to capture internal changes and propagate these changes downstream. When used in conjunction with Delta Lake, Autoloader enables the reading of incremental files from an S3 location and simplifies the process of upserting them into downstream Delta tables. To enhance efficiency, we are moving away from using CSV storage for CDC information and opting to store it in a Delta table. For example, below code will ingest the incremental files from the S3 location to a bronze table using Autoloader: checkpoint_path = \"/Volumes/<catalog_name>/<bronze_schema_name>/<volume_name>/<sub_path_bronze_table>\" bronzeDF = (spark.readStream .format(\"cloudFiles\") .option(\"cloudFiles.format\", \"CSV\") .option(\"header\", \"true\") .option(\"cloudFiles.schemaLocation\", checkpoint_path) .load(\"s3://<landing-bucket>/<path-to-files>\")) (bronzeDF.writeStream .option(\"checkpointLocation\", checkpoint_path) .trigger(availableNow=True) .toTable(\"<catalog_name>.<bronze_schema_name>.<cdc_bronze_table_name>\")) Define the MERGE statement to upsert the CDC information in our final table. Please be aware that the use of \"DELETE\" is discretionary; only employ it when an actual deletion is occurring at the source. Alternatively, if the source incorporates an active or inactive flag, the \"UPDATE\" action will adequately address the situation. sql_query = \"\"\" MERGE INTO <silver_table_name> a USING stream_updates b ON a.<key_field> = b.<key_field> WHEN MATCHED AND b.<OPERATION_FIELD> = 'U' THEN UPDATE SET * WHEN NOT MATCHED AND b.<OPERATION_FIELD> = 'C' THEN INSERT * WHEN MATCHED AND b.<OPERATION_FIELD> = 'D' THEN DELETE For each batch / incremental update from the raw bronze table, this function will run a MERGE on the silver table. def merge_stream(df, i): df.createOrReplaceTempView(\"stream_updates\") df._jdf.sparkSession().sql(sql_query) For finalizing the process, create a silver table: %sql use catalog <catalog_name>; use schema <silver_schema_name>; CREATE TABLE IF NOT EXISTS <silver_table_name> AS SELECT * FROM <bronze_schema_name>.<cdc_bronze_table_name> WHERE 1=2 Retrieve data from the bronze table with delta changes Upsert-Merge it into the silver table using the previously defined merge_stream function: checkpoint_path = \"/Volumes/<catalog_name>/<silver_schema_name>/<volume_name>/<sub_path_silver_table>\" (spark .readStream.table(\"<cdc_bronze_table_name>\") .filter(\"<filter_condition>\") .writeStream .foreachBatch(merge_stream) .option(\"checkpointLocation\", checkpoint_path_silver) .trigger(availableNow=True) .start())","title":"***SAP BW Data Incremental Ingestion to LEGO Nexus***"},{"location":"ingestion/ingestion-patterns/SAP-BW-Ingestion-pattern/#sap-bw-data-incremental-ingestion-to-lego-nexus","text":"","title":"SAP BW Data Incremental Ingestion to LEGO Nexus"},{"location":"ingestion/ingestion-patterns/SAP-BW-Ingestion-pattern/#prerequisites","text":"Review the principles and guidelines to publish data from SAP BW to Lego Nexus outlined in data-matters Baseplate document. Data products should be registered using the data product registration template as described in the registration guide . If a BI engineering product team doesn't have an application in Architecture Gateway , they need to register one (or more if necessary) so that the source of the data is correctly represented and linked to the correct digital product. User guide to Create an application in Architecture Gateway. Prior to commencing the implementation of the CDC pipeline in Databricks, it is essential to configure the S3 bucket and establish a connection by linking your S3 bucket through the External location. The process for bringing your own bucket, requesting storage credentials and external location for a S3 bucket is outlined in detail within the Baseplate BYOB document.","title":"Prerequisites"},{"location":"ingestion/ingestion-patterns/SAP-BW-Ingestion-pattern/#architectural-flow-diagram","text":"","title":"Architectural flow diagram"},{"location":"ingestion/ingestion-patterns/SAP-BW-Ingestion-pattern/#incremental-extraction-and-landing-it-in-s3","text":"","title":"Incremental Extraction and landing it in S3"},{"location":"ingestion/ingestion-patterns/SAP-BW-Ingestion-pattern/#operational-data-provisioning-odp-and-delta-queue-odq-based-extraction","text":"Operational Data Provisioning (ODP) supports extraction and replication scenarios for various target applications and incorporates delta mechanisms in these scenarios. When dealing with a delta procedure, data from a source (referred to as the ODP Provider) is automatically written to a delta queue, known as the Operational Delta Queue (ODQ), through an update process or passed to the delta queue via an extractor interface. The ODQ effectively tracks new and modified records since the last extraction, facilitating incremental data updates.","title":"Operational Data Provisioning (ODP) and Delta Queue (ODQ) based Extraction :"},{"location":"ingestion/ingestion-patterns/SAP-BW-Ingestion-pattern/#odp-context-or-provider","text":"An ODP context represents a source of ODPs. Context identifiers are present for all technologies whose analytical views can be exposed as ODPs. Currently, the following ODP contexts are available (depending on release): Provider Name Description ABAP_CDS ABAP Core Data Services BW SAP NetWeaver / BW/4HANA BYD SAP Business ByDesign. Data from MDAV extracted via ODP. Implemented via SOAP Web Service. ESH Search and operational analytics HANA SAP HANA Information View SAPI SAP Service Application Programming Interface (S-API) for SAP DataSources SLT~ SAP Landscape Transformation Replication Server In our extraction process, we will utilize the BW context and retrieve data from the BW DataStore .","title":"ODP Context or Provider"},{"location":"ingestion/ingestion-patterns/SAP-BW-Ingestion-pattern/#odp-subscribers-or-consumers","text":"Currently, the following subscriber types are available (depending on release): Subscribers Description SAP_BW/4HANA SAP NetWeaver Business Warehouse DS SAP Business Objects Data Services TREX_ES SAP NetWeaver Embedded Analytics. Query is defined on transient provider, derived from ODP RODPS_REPL_TEST Created by executing report RODPS_REPL_TEST (in transaction SE38) RSODP_ODATA Open Data Protocol (OData) HANA_SDI SAP HANA Smart Data Integration We will be using SAP Data Services as consumer.","title":"ODP Subscribers or Consumers"},{"location":"ingestion/ingestion-patterns/SAP-BW-Ingestion-pattern/#extraction-using-sap-data-services","text":"Data Services has seamless integration capabilities with SAP ODP-enabled data sources. In Lego BW system, most of the BW data store objects(DSO) are ODP enabled, and SAP Data Services can use extractors for data extraction through the ODP API framework. To extract data from the BW system using extractors, you should have a Datastore configured as the source in Data Services with BW ODP context. To import the required ODP-enabled object, follow these steps: Double-click on the Datastore to view the list of ODP-enabled objects. Identify the necessary ODP extractors from the list. Right-click on the desired ODP extractor and choose the \"Import\" option. During import provide the consumer name and project name and CDC Mode . When importing any ODP, one can choose between query mode or CDC mode. For incremental extraction choose CDC. Each of these projects will be a separate subscription in the source system, which you can view in the T-CODE ODQMON. If the file location isn't already created for that S3 landing bucket then reate a cloud storage file location to land the file in the landing S3 bucket. The team taking the role of the Data Producer has their own AWS Account. If the S3 bucket for landing data hasn't been created yet, please refer to the 'Create AWS Bucket' section in the Baseplate BYOB document. create an IAM user in the AWS account and Set permissions to read and write files from S3. Before the DataFlow, a scipt can be used to create CSV file names with a dynamic timestamp. Create a Data Services data flow that uses an ODP (Operational Data Provisioning) object as the source and exports the data to a specified file location. The way we use MAP CDC Operation depends on what we need \u2013 whether to send all the changes that happened over time or just the current information. This choice is based on the type of extractor we're using. The \"File name(s)\" points output of the script in the step no 5 to create a dynamic file name with a timestamp ($Name1) If the initial load is set to 'Yes,' it will import all the data during the first execution. Subsequently, for incremental loads, it should be set to 'No' after the initial execution. The sample output file contains a column labeled ODQ_CHANGEMODE , serving as an indicator for operational types. The potential values include: C - New Record U - Updated Record D - Deleted Record","title":"Extraction using SAP Data Services :"},{"location":"ingestion/ingestion-patterns/SAP-BW-Ingestion-pattern/#data-load-into-lego-nexus-bronze-table-from-s3","text":"","title":"Data Load into LEGO Nexus Bronze table from S3"},{"location":"ingestion/ingestion-patterns/SAP-BW-Ingestion-pattern/#implement-cdc-change-data-capture-pipeline-with-delta-lake","text":"Delta Lake is designed to support CDC workloads by offering support for UPDATE, DELETE, and MERGE operations. Additionally, Delta tables can facilitate CDC to capture internal changes and propagate these changes downstream. When used in conjunction with Delta Lake, Autoloader enables the reading of incremental files from an S3 location and simplifies the process of upserting them into downstream Delta tables. To enhance efficiency, we are moving away from using CSV storage for CDC information and opting to store it in a Delta table. For example, below code will ingest the incremental files from the S3 location to a bronze table using Autoloader: checkpoint_path = \"/Volumes/<catalog_name>/<bronze_schema_name>/<volume_name>/<sub_path_bronze_table>\" bronzeDF = (spark.readStream .format(\"cloudFiles\") .option(\"cloudFiles.format\", \"CSV\") .option(\"header\", \"true\") .option(\"cloudFiles.schemaLocation\", checkpoint_path) .load(\"s3://<landing-bucket>/<path-to-files>\")) (bronzeDF.writeStream .option(\"checkpointLocation\", checkpoint_path) .trigger(availableNow=True) .toTable(\"<catalog_name>.<bronze_schema_name>.<cdc_bronze_table_name>\")) Define the MERGE statement to upsert the CDC information in our final table. Please be aware that the use of \"DELETE\" is discretionary; only employ it when an actual deletion is occurring at the source. Alternatively, if the source incorporates an active or inactive flag, the \"UPDATE\" action will adequately address the situation. sql_query = \"\"\" MERGE INTO <silver_table_name> a USING stream_updates b ON a.<key_field> = b.<key_field> WHEN MATCHED AND b.<OPERATION_FIELD> = 'U' THEN UPDATE SET * WHEN NOT MATCHED AND b.<OPERATION_FIELD> = 'C' THEN INSERT * WHEN MATCHED AND b.<OPERATION_FIELD> = 'D' THEN DELETE For each batch / incremental update from the raw bronze table, this function will run a MERGE on the silver table. def merge_stream(df, i): df.createOrReplaceTempView(\"stream_updates\") df._jdf.sparkSession().sql(sql_query) For finalizing the process, create a silver table: %sql use catalog <catalog_name>; use schema <silver_schema_name>; CREATE TABLE IF NOT EXISTS <silver_table_name> AS SELECT * FROM <bronze_schema_name>.<cdc_bronze_table_name> WHERE 1=2 Retrieve data from the bronze table with delta changes Upsert-Merge it into the silver table using the previously defined merge_stream function: checkpoint_path = \"/Volumes/<catalog_name>/<silver_schema_name>/<volume_name>/<sub_path_silver_table>\" (spark .readStream.table(\"<cdc_bronze_table_name>\") .filter(\"<filter_condition>\") .writeStream .foreachBatch(merge_stream) .option(\"checkpointLocation\", checkpoint_path_silver) .trigger(availableNow=True) .start())","title":"Implement CDC (Change Data Capture) Pipeline With Delta Lake :"}]}